{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b062d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '20230329'\n",
    "to_date = '20230329'\n",
    "searching_word = 'finance OR banking OR investment OR economy OR sustainability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import re\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_autoinstaller\n",
    "import subprocess\n",
    "import shutil\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from newspaper import Article\n",
    "\n",
    "import nltk\n",
    "from keybert import KeyBERT\n",
    "\n",
    "from collections import Counter\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# 워드 클라우드\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import (WordCloud, get_single_color_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 환경설정 및 세팅\n",
    "try:\n",
    "    shutil.rmtree(r\"c:\\chrometemp\")  #쿠키 / 캐쉬파일 삭제\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "subprocess.Popen(r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe --remote-debugging-port=9222 --user-data-dir=\"C:\\chrometemp\"') # 디버거 크롬 구동\n",
    "\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]\n",
    "try:\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "except:\n",
    "    chromedriver_autoinstaller.install(True)\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "driver.implicitly_wait(10)\n",
    "cwd = os.getcwd()\n",
    "main_path = cwd + '/' + searching_word\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "createFolder(main_path)\n",
    "createFolder(main_path + '/' + 'url')\n",
    "createFolder(main_path + '/' + 'news_backup')\n",
    "createFolder(main_path + '/' + 'news')\n",
    "createFolder(main_path + '/' + 'keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 키워드 검색 뉴스 url 수집\n",
    "def page2url(searching_word,date):\n",
    "    searching_word = '{}'.format(searching_word)\n",
    "    urls = []\n",
    "    for start in range(0, 360, 10): ### 하루에 관련 기사를 최대 얼마나 뽑을지 설정.\n",
    "        main_url = 'https://www.google.co.kr/search?q={}&tbs=cdr:1,cd_min:{},cd_max:{}&tbm=nws&ei=dPP-Yu_eCJLL-Qb55bvQDA&start={}&sa=N&ved=2ahUKEwjv6Lvy69H5AhWSZd4KHfnyDso4ChDy0wN6BAgBEDk&biw=1536&bih=754&dpr=1.25'.format(searching_word,date,date,start)\n",
    "        driver.get(url=main_url)\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        lnks = []\n",
    "        for lnk in elements:\n",
    "            lnk = str(lnk.get_attribute('href'))\n",
    "            if 'google' not in lnk and lnk != 'None':\n",
    "                lnks.append(lnk)\n",
    "        if len(lnks) == 0:\n",
    "            print(date, ' Url 수집 완료')\n",
    "            break\n",
    "        urls.extend(lnks)\n",
    "        rand_value =random.uniform(4, 10)\n",
    "        time.sleep(rand_value)\n",
    "        \n",
    "    return urls\n",
    "\n",
    "datelist = pd.date_range(start=from_date, end=to_date).tolist()\n",
    "dtlst = []\n",
    "for d_t in datelist:\n",
    "    d_t = str(d_t)[0:-9]\n",
    "    d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "    d = d.strftime('%m/%d/%Y')\n",
    "    d = d[0].replace('0','') + d[1:]\n",
    "    d = d[:-7] + d[-7].replace('0','') + d[-6:]\n",
    "    dtlst.append(d)\n",
    "    \n",
    "    \n",
    "Urls = dict()\n",
    "for date in tqdm(dtlst):\n",
    "    print(date,'Url 수집 시작')\n",
    "    urls = page2url(searching_word,date)\n",
    "    Urls[date] = urls\n",
    "    \n",
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'url/' + name,'wb') as f:\n",
    "    pickle.dump(Urls,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82183f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2  수집된 url로 news 크롤링 \n",
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'url/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# start = 0\n",
    "for date in list(data):\n",
    "    print(date, '크롤링 시작')\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    urls = []\n",
    "    for idx,url in tqdm(enumerate(data[date])):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            v = article.text\n",
    "            tmp.append(v)\n",
    "            urls.append(url)\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "            tmp.append(\"null\")\n",
    "            urls.append(url)\n",
    "            print(url)\n",
    "    print(\"총 {}건 중 {} 건 크롤링 성공\".format(idx+1,cnt))\n",
    "    data[date] = [tmp,[urls]]\n",
    "    date = date.replace('/','-')\n",
    "    with open(main_path + '/' + 'news_backup/' + 'news-' + date + '.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    print(date,' 크롤링 완료')\n",
    "    \n",
    "data = pd.DataFrame(data).T\n",
    "\n",
    "data = data.rename({0:\"news\",1:\"url\"},axis= 1)\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 뉴스기사에서 키워드 추출\n",
    "\n",
    "def doc2key(cleaned_content):\n",
    "    \n",
    "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "    \n",
    "    n2_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(2, 2), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=1)\n",
    "\n",
    "    n1_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(1, 1), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=20)\n",
    "    for idx,i in enumerate(n2_kwd):\n",
    "        n2_kwd[idx] = i[0]\n",
    "    for idx,i in enumerate(n1_kwd):\n",
    "        n1_kwd[idx] = i[0]  \n",
    "\n",
    "    n1_kwd.extend(n2_kwd)\n",
    "    kwd = n1_kwd\n",
    "    return kwd\n",
    "\n",
    "def makeKeywordLsit(data,nn_jj = False):\n",
    "    kwd_list = []\n",
    "    for docs in tqdm(data['news']):\n",
    "        kwds = []\n",
    "        for doc in tqdm(docs):\n",
    "            try:\n",
    "                cleaned_content = re.sub(r'[^\\.\\?\\!\\w\\d\\s]','',doc) # 문장단위로 끊기\n",
    "                cleaned_content = cleaned_content.replace('\\n',' ')\n",
    "                cleaned_content = cleaned_content.lower()\n",
    "                kwd = doc2key(cleaned_content)\n",
    "                if nn_jj == True:\n",
    "                    tokens_pos = nltk.pos_tag(kwd)\n",
    "                    kwd_nn_jj = []\n",
    "                    for word, pos in tokens_pos:\n",
    "                        if 'NN' in pos or 'JJ' in pos:\n",
    "                            kwd_nn_jj.append(word)\n",
    "                    kwds.append(kwd_nn_jj)\n",
    "                else:\n",
    "                    kwds.append(kwd)\n",
    "            except:\n",
    "                print(doc)\n",
    "        kwd_list.append(kwds)\n",
    "    return kwd_list\n",
    "\n",
    "# 데이터불러오기\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# 전처리\n",
    "data = data.reset_index()\n",
    "data = data.rename({\"index\":\"date\"},axis = 1)\n",
    "\n",
    "idx = []\n",
    "for x in data['date']:\n",
    "    tmp = x.split('/')\n",
    "    if len(tmp[0]) == 1:\n",
    "        tmp[0] = '0'+tmp[0]\n",
    "    if len(tmp[1]) == 1:\n",
    "        tmp[1] = '0'+tmp[1]\n",
    "    tmp = tmp[2] + tmp[0] + tmp[1]\n",
    "    idx.append(''.join(tmp))\n",
    "    \n",
    "data['date'] = idx\n",
    "data = data.rename({'date':'일자'},axis = 1)\n",
    "\n",
    "# 키워드 추출\n",
    "kwd_list = makeKeywordLsit(data, nn_jj = True)\n",
    "\n",
    "# 데이터 추가\n",
    "data['키워드'] = kwd_list\n",
    "\n",
    "# 저장\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7abd72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'searching_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9348\\2423390509.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mdf_es\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msearching_word\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'Efficient_OLS_Keyword {}-{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8-sig\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m \u001b[0mDataEn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearching_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mslope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 한달이내는 기울기를 만들 수 없음.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'searching_word' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. 추출된 키워드 전처리\n",
    "def get_slop(df2,from_date,to_date):\n",
    "    datelist = pd.date_range(start=from_date, end=to_date, freq = 'M').tolist()\n",
    "    dtlst = []\n",
    "    df = df2.copy()\n",
    "    for d_t in datelist:\n",
    "        d_t = str(d_t)[0:-9]\n",
    "        d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "        d = d.strftime('%Y%m')\n",
    "        dtlst.append(d)\n",
    "    for dt in dtlst:\n",
    "        df[dt] = df[dt][:-1]*10000 / df[dt][-1]\n",
    "    from scipy import stats\n",
    "\n",
    "    x = [x + 1for x in range(0,len(dtlst))]\n",
    "\n",
    "    slopes = []\n",
    "    for i in list(df.values):\n",
    "        y = i[:-2]\n",
    "        slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
    "        slopes.append(slope)\n",
    "    df['slope'] = slopes\n",
    "    return df\n",
    "\n",
    "def data_pro(data,from_date,to_date):\n",
    "\n",
    "    df = data[['일자','키워드']]\n",
    "    df['일자'] = [str(x)[0:6] for x in df['일자']] \n",
    "    # Input string\n",
    "    date_str = to_date\n",
    "\n",
    "    # Convert the string to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "    # Add one month to the datetime object\n",
    "    new_date_obj = date_obj + relativedelta(months=1)\n",
    "\n",
    "    # Convert the result back to a string\n",
    "    new_date_str = new_date_obj.strftime('%Y%m%d')\n",
    "    to_date = new_date_str\n",
    "    datelist = pd.date_range(start=from_date, end=to_date, freq = 'M').tolist()\n",
    "    dtlst = []\n",
    "\n",
    "    for d_t in datelist:\n",
    "        d_t = str(d_t)[0:-9]\n",
    "        d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "        d = d.strftime('%Y%m')\n",
    "        dtlst.append(d)\n",
    "\n",
    "    # 키워드 사전 \n",
    "    keyword_vocab = []\n",
    "    for i in df['키워드']:\n",
    "        keyword_vocab.extend(i)\n",
    "    keyword_vocab = list(set(keyword_vocab))\n",
    "\n",
    "    # 월별 카운팅 후 병합 # 병합하는 방식 수정해야함. 한달만 카운트는 현재 불가.\n",
    "\n",
    "    tmp_df_list = []\n",
    "    for dt in dtlst:\n",
    "        tmp = []\n",
    "        for i in df[df['일자']== dt]['키워드']:\n",
    "            tmp.extend(i)\n",
    "        tmp_df = pd.DataFrame(pd.Series(Counter(tmp)))\n",
    "        tmp_df = tmp_df.reset_index()\n",
    "        tmp_df = tmp_df.rename(columns = {0:'cnt','index':'keyword'})\n",
    "        tmp_df_list.append(tmp_df)\n",
    "\n",
    "    if len(tmp_df_list) > 1:\n",
    "        df = tmp_df_list[0].merge(tmp_df_list[1], on = 'keyword', how = 'outer')\n",
    "        for i in range(len(tmp_df_list)):\n",
    "            if i > 1:\n",
    "                df = df.merge(tmp_df_list[i], on = 'keyword', how = 'outer')\n",
    "        col = ['keyword'] + dtlst\n",
    "        df.columns = col\n",
    "    elif len(tmp_df_list) == 1:\n",
    "        df = tmp_df_list[0]\n",
    "        col = ['keyword'] + dtlst\n",
    "        df.columns = col\n",
    "\n",
    "    # 합산 값 만들기\n",
    "    df.index = df['keyword']\n",
    "    df = df.drop('keyword', axis = 1)\n",
    "    df['tot_cnt'] = df.sum(axis = 1)\n",
    "    df = df.T\n",
    "    df['year_cnt'] = df.sum(axis = 1)\n",
    "    df = df.T\n",
    "    df\n",
    "\n",
    "    # 결측치 처리\n",
    "    df =df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def DataEn(searching_word,from_date,to_date,slope = True):\n",
    "    cwd = os.getcwd()\n",
    "    main_path = cwd + '/' + searching_word\n",
    "    name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "    name = name.replace('/','.')\n",
    "    with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "        data_e = pickle.load(f)\n",
    "      \n",
    "    result = data_e[['일자','키워드']].explode('키워드').reset_index(drop=True)\n",
    "    data_e = result\n",
    "    print(\"총 기사 건 수 : \" + str(len(data_e['키워드'])))\n",
    "    df_e = data_pro(data_e,from_date,to_date)\n",
    "    df_e.to_csv(cwd +'/' + searching_word + '/'+ 'Monthly_Count_Keyword {}-{}.csv'.format(from_date,to_date),encoding=\"utf-8-sig\")\n",
    "    \n",
    "    if slope == True:\n",
    "        df_es = get_slop(df_e,from_date,to_date)\n",
    "        df_es.to_csv(cwd +'/' + searching_word + '/'+ 'Efficient_OLS_Keyword {}-{}.csv'.format(from_date,to_date),encoding=\"utf-8-sig\")\n",
    "        \n",
    "DataEn(searching_word,from_date,to_date,slope = False) # 한달이내는 기울기를 만들 수 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 워드클라우드 그리기\n",
    "\n",
    "def DrawPointColoredWC(tags,title,cwd,searching_word, drop_list=[], color = '#00ff00', pointed_list=[]):\n",
    "    color_to_words = {\n",
    "        color : pointed_list\n",
    "    }\n",
    "\n",
    "    use_tags = tags.drop(drop_list, errors = 'ignore')\n",
    "    wc = WordCloud(font_path='C:/Windows/Fonts/malgun',background_color=\"white\",width=1600, height=800,random_state = 1)\n",
    "\n",
    "    default_color = 'black'\n",
    "    grouped_color_func = GroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "    cloud = wc.generate_from_frequencies(dict(use_tags))\n",
    "    wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "    figure(dpi=1200)\n",
    "    figure(figsize=[12,8])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.savefig(cwd + '/' + searching_word + '/'+ title + ' 강조'+'.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n",
    "\n",
    "\n",
    "class GroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n",
    "       specified colors to certain words based on the color to words mapping.\n",
    "\n",
    "       Uses wordcloud.get_single_color_func\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)\n",
    "    \n",
    "# 데이터 전처리\n",
    "cwd = os.getcwd()\n",
    "path = cwd +'/' + searching_word + '/raw'\n",
    "\n",
    "df_cnt = pd.read_csv(cwd +'/' + searching_word + '/'+ 'Monthly_Count_Keyword {}-{}.csv'.format(from_date,to_date),index_col = 'keyword')\n",
    "\n",
    "main_path = cwd + '/' + searching_word\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "    data_e = pickle.load(f)\n",
    "    \n",
    "result = data_e[['일자','url']].explode('url').reset_index(drop=True)\n",
    "url_long_data = result.explode('url').reset_index(drop=True)\n",
    "\n",
    "keyword_long_data = data_e[['일자','키워드']].explode('키워드').reset_index(drop=True)\n",
    "\n",
    "print('{}로 검색하여 {}부터 {}까지 {} 건의 기사를 인용했습니다.'.format(searching_word,from_date,to_date,len(url_long_data )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a884cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 키워드 수, 지울 단어, 강조할 단어 지정\n",
    "num_exp = 50\n",
    "drop_list = [np.nan,'wednesday','december','today','monday','thomson','httpswww','sustainable','sustainability','review','service','supports','review terms','sure','information','reuters','policy','terms','theres','happen','cookies','content','javascript','average','sector','sectors','standards','trust','browser','supports javascript','did happen','loading information']\n",
    "num = num_exp + len(drop_list)\n",
    "color_to_words = {\n",
    "    '#00ff00': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcaf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그림 그리기 및 저장\n",
    "print('{}에서 {}까지 키워드 출현 빈도 순 상위 {}개 입니다'.format(from_date,to_date,num_exp))\n",
    "tags = df_cnt.sort_values(by = 'tot_cnt', ascending = False)[1:num+1]['tot_cnt']\n",
    "tags = tags.drop(drop_list, errors = 'ignore')\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun',background_color=\"white\",width=1600, height=800,random_state = 1)\n",
    "# default_color = 'black'\n",
    "# grouped_color_func = GroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "cloud = wc.generate_from_frequencies(dict(tags))\n",
    "# wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "figure(dpi=1200)\n",
    "title = '대상기간_최대출현_키워드_{}개'.format(str(num_exp))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.savefig(cwd + '/' + searching_word + '/'+ title + from_date + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'savings'\n",
    "\n",
    "tmp = []\n",
    "for i in keyword_long_data['키워드']:\n",
    "    if keyword in i:\n",
    "        tmp.append(True)\n",
    "    else:\n",
    "        tmp.append(False)\n",
    "\n",
    "for url in url_long_data[tmp]['url']:\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
