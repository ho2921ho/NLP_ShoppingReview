{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50845f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '20230327'\n",
    "to_date = '20230327'\n",
    "searching_word = 'finance OR banking OR investment OR economy'\n",
    "topic = 'finance OR banking OR investment OR economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2170b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import re\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from newspaper import Article\n",
    "\n",
    "import nltk\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3f7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NH\\AppData\\Local\\Temp\\ipykernel_9724\\1447849451.py:19: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_autoinstaller\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(r\"c:\\chrometemp\")  #쿠키 / 캐쉬파일 삭제\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "subprocess.Popen(r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe --remote-debugging-port=9222 --user-data-dir=\"C:\\chrometemp\"') # 디버거 크롬 구동\n",
    "\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]\n",
    "try:\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "except:\n",
    "    chromedriver_autoinstaller.install(True)\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab69c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "main_path = cwd + '/' + searching_word\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "createFolder(main_path)\n",
    "createFolder(main_path + '/' + 'url')\n",
    "createFolder(main_path + '/' + 'news_backup')\n",
    "createFolder(main_path + '/' + 'news')\n",
    "createFolder(main_path + '/' + 'keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7276dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/27/2023 Url 수집 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [03:17<00:00, 197.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/27/2023  Url 수집 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 키워드 검색 뉴스 url 수집\n",
    "def page2url(searching_word,date):\n",
    "    searching_word = '{}'.format(searching_word)\n",
    "    urls = []\n",
    "    for start in range(0, 360, 10): ### 하루에 관련 기사를 최대 얼마나 뽑을지 설정.\n",
    "        main_url = 'https://www.google.co.kr/search?q={}&tbs=cdr:1,cd_min:{},cd_max:{}&tbm=nws&ei=dPP-Yu_eCJLL-Qb55bvQDA&start={}&sa=N&ved=2ahUKEwjv6Lvy69H5AhWSZd4KHfnyDso4ChDy0wN6BAgBEDk&biw=1536&bih=754&dpr=1.25'.format(searching_word,date,date,start)\n",
    "        driver.get(url=main_url)\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        lnks = []\n",
    "        for lnk in elements:\n",
    "            lnk = str(lnk.get_attribute('href'))\n",
    "            if 'google' not in lnk and lnk != 'None':\n",
    "                lnks.append(lnk)\n",
    "        if len(lnks) == 0:\n",
    "            print(date, ' Url 수집 완료')\n",
    "            break\n",
    "        urls.extend(lnks)\n",
    "        rand_value =random.uniform(4, 10)\n",
    "        time.sleep(rand_value)\n",
    "        \n",
    "    return urls\n",
    "\n",
    "datelist = pd.date_range(start=from_date, end=to_date).tolist()\n",
    "dtlst = []\n",
    "for d_t in datelist:\n",
    "    d_t = str(d_t)[0:-9]\n",
    "    d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "    d = d.strftime('%m/%d/%Y')\n",
    "    d = d[0].replace('0','') + d[1:]\n",
    "    d = d[:-7] + d[-7].replace('0','') + d[-6:]\n",
    "    dtlst.append(d)\n",
    "    \n",
    "    \n",
    "Urls = dict()\n",
    "for date in tqdm(dtlst):\n",
    "    print(date,'Url 수집 시작')\n",
    "    urls = page2url(searching_word,date)\n",
    "    Urls[date] = urls\n",
    "    \n",
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'url/' + name,'wb') as f:\n",
    "    pickle.dump(Urls,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff42ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/27/2023 크롤링 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:33,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://linknky.com/news/2023/03/27/republic-bank-continues-expansion-into-northern-kentucky-with-opening-of-bellevue-banking-center/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:47,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://focustaiwan.tw/business/202303270020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [01:17,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gov.wales/economy-minister-congratulates-celtic-freeport-consortium-winning-bid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [01:23,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pymnts.com/digital-first-banking/2023/pymnts-intelligence-why-digital-everywhere-is-moving-banking-forward/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [01:34,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forbes.com/sites/forbesbusinesscouncil/2023/03/27/how-gulf-cooperation-council-initiatives-are-shifting-the-foreign-investment-landscape/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [02:05,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://africanfinancials.com/document/sz-sprop-2023-ir-hy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [02:08,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.economy-ni.gov.uk/publications/dfe-resource-budget-2022-2023-equality-screening\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [02:14,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thefintechtimes.com/arab-jordan-investment-bank-purchases-standard-chartereds-banking-business-in-jordan/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [02:28,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nashville.gov/departments/council/boards/metro-council/meetings/metropolitan-council-special-joint-meeting-budget-and-finance-committee-and-east-bank-stadium-committee-meeting-march-27-2023-cancelled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [02:39,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.barrons.com/articles/the-latest-in-the-banking-sector-turmoil-d54c10a8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [03:01,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pymnts.com/acquisitions/2023/visa-is-competing-acquire-banking-payments-platform-pismo/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [03:13,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wsj.com/articles/banks-step-up-to-serve-crypto-firms-after-signature-silvergate-blowups-5e7b4074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [03:27,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.hedgeweek.com/2023/03/27/319983/groucho-marx-and-investment-management-lessons-manager-selection\n",
      "https://www.pymnts.com/gig-economy/2023/consumer-pullback-threatens-paycheck-to-paycheck-gig-economy-lifelines/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "181it [03:27,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.outlookindia.com/business-spotlight/solana-fails-to-match-ethereum-while-renq-finance-almost-guaranteed-to-outplace-chainlink-news-273657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "195it [03:44,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pymnts.com/news/b2b-payments/2023/energy-company-eni-launches-esg-focused-supply-chain-finance-program/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [03:56,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.barrons.com/articles/asian-markets-mixed-as-banking-fears-persist-e4ac71e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224it [04:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.freemalaysiatoday.com/category/nation/2023/03/27/anwar-to-announce-major-investment-this-week/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [04:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 226건 중 208 건 크롤링 성공\n",
      "3-27-2023  크롤링 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "# 2  수집된 url로 news 크롤링 \n",
    "with open(main_path + '/' + 'url/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# start = 0\n",
    "for date in list(data):\n",
    "    print(date, '크롤링 시작')\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    urls = []\n",
    "    for idx,url in tqdm(enumerate(data[date])):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            v = article.text\n",
    "            tmp.append(v)\n",
    "            urls.append(url)\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "            tmp.append(\"null\")\n",
    "            urls.append(url)\n",
    "            print(url)\n",
    "    print(\"총 {}건 중 {} 건 크롤링 성공\".format(idx+1,cnt))\n",
    "    data[date] = [tmp,[urls]]\n",
    "    date = date.replace('/','-')\n",
    "    with open(main_path + '/' + 'news_backup/' + 'news-' + date + '.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    print(date,' 크롤링 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b7f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data).T\n",
    "\n",
    "data = data.rename({0:\"news\",1:\"url\"},axis= 1)\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc31b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2key(cleaned_content):\n",
    "    \n",
    "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "    keywords = kw_model.extract_keywords(cleaned_content)\n",
    "    \n",
    "    n2_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(2, 2), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=5)\n",
    "\n",
    "    n1_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(1, 1), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=45)\n",
    "    for idx,i in enumerate(n2_kwd):\n",
    "        n2_kwd[idx] = i[0]\n",
    "    for idx,i in enumerate(n1_kwd):\n",
    "        n1_kwd[idx] = i[0]  \n",
    "\n",
    "    n1_kwd.extend(n2_kwd)\n",
    "    kwd = n1_kwd\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de4f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeKeywordLsit(data,nn_jj = False):\n",
    "    kwd_list = []\n",
    "    for docs in tqdm(data['news']):\n",
    "        kwds = []\n",
    "        for doc in tqdm(docs):\n",
    "            try:\n",
    "                cleaned_content = re.sub(r'[^\\.\\?\\!\\w\\d\\s]','',doc) # 문장단위로 끊기\n",
    "                cleaned_content = cleaned_content.replace('\\n',' ')\n",
    "                cleaned_content = cleaned_content.lower()\n",
    "                kwd = doc2key(cleaned_content)\n",
    "                if nn_jj == True:\n",
    "                    tokens_pos = nltk.pos_tag(kwd)\n",
    "                    kwd_nn_jj = []\n",
    "                    for word, pos in tokens_pos:\n",
    "                        if 'NN' in pos or 'JJ' in pos:\n",
    "                            kwd_nn_jj.append(word)\n",
    "                    kwds.append(kwd_nn_jj)\n",
    "                else:\n",
    "                    kwds.append(kwd)\n",
    "            except:\n",
    "                print(doc)\n",
    "        kwd_list.append(kwds)\n",
    "    return kwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da89f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터불러오기\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab5c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "data = data.reset_index()\n",
    "data = data.rename({\"index\":\"date\"},axis = 1)\n",
    "\n",
    "idx = []\n",
    "for x in data['date']:\n",
    "    tmp = x.split('/')\n",
    "    if len(tmp[0]) == 1:\n",
    "        tmp[0] = '0'+tmp[0]\n",
    "    if len(tmp[1]) == 1:\n",
    "        tmp[1] = '0'+tmp[1]\n",
    "    tmp = tmp[2] + tmp[0] + tmp[1]\n",
    "    idx.append(''.join(tmp))\n",
    "    \n",
    "data['date'] = idx\n",
    "data = data.rename({'date':'일자'},axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                          | 0/226 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▎                                                                                 | 1/226 [00:02<09:37,  2.57s/it]\u001b[A\n",
      "  1%|▋                                                                                 | 2/226 [00:04<08:24,  2.25s/it]\u001b[A\n",
      "  1%|█                                                                                 | 3/226 [00:06<08:19,  2.24s/it]\u001b[A\n",
      "  2%|█▍                                                                                | 4/226 [00:08<08:06,  2.19s/it]\u001b[A\n",
      "  2%|█▊                                                                                | 5/226 [00:10<07:09,  1.94s/it]\u001b[A\n",
      "  3%|██▏                                                                               | 6/226 [00:13<08:14,  2.25s/it]\u001b[A\n",
      "  3%|██▌                                                                               | 7/226 [00:13<06:03,  1.66s/it]\u001b[A\n",
      "  4%|██▉                                                                               | 8/226 [00:14<05:19,  1.47s/it]\u001b[A\n",
      "  4%|███▎                                                                              | 9/226 [00:15<04:43,  1.30s/it]\u001b[A\n",
      "  4%|███▌                                                                             | 10/226 [00:17<05:05,  1.41s/it]\u001b[A\n",
      "  5%|███▉                                                                             | 11/226 [00:19<05:34,  1.55s/it]\u001b[A\n",
      "  5%|████▎                                                                            | 12/226 [00:20<04:50,  1.36s/it]\u001b[A\n",
      "  6%|████▋                                                                            | 13/226 [00:20<04:00,  1.13s/it]\u001b[A\n",
      "  6%|█████                                                                            | 14/226 [00:23<05:27,  1.54s/it]\u001b[A\n",
      "  7%|█████▍                                                                           | 15/226 [00:24<05:00,  1.42s/it]\u001b[A\n",
      "  7%|█████▋                                                                           | 16/226 [00:26<05:13,  1.49s/it]\u001b[A\n",
      "  8%|██████                                                                           | 17/226 [00:26<04:21,  1.25s/it]\u001b[A\n",
      "  8%|██████▍                                                                          | 18/226 [00:27<04:00,  1.16s/it]\u001b[A\n",
      "  8%|██████▊                                                                          | 19/226 [00:29<04:21,  1.26s/it]\u001b[A\n",
      "  9%|███████▏                                                                         | 20/226 [00:31<04:57,  1.44s/it]\u001b[A\n",
      "  9%|███████▌                                                                         | 21/226 [00:32<05:19,  1.56s/it]\u001b[A\n",
      " 10%|███████▉                                                                         | 22/226 [00:34<05:43,  1.69s/it]\u001b[A\n",
      " 10%|████████▏                                                                        | 23/226 [00:36<05:27,  1.61s/it]\u001b[A\n",
      " 11%|████████▌                                                                        | 24/226 [00:37<04:58,  1.48s/it]\u001b[A\n",
      " 11%|████████▉                                                                        | 25/226 [00:39<05:48,  1.74s/it]\u001b[A\n",
      " 12%|█████████▎                                                                       | 26/226 [00:40<05:07,  1.54s/it]\u001b[A\n",
      " 12%|█████████▋                                                                       | 27/226 [00:43<06:22,  1.92s/it]\u001b[A\n",
      " 12%|██████████                                                                       | 28/226 [00:48<08:43,  2.64s/it]\u001b[A\n",
      " 13%|██████████▍                                                                      | 29/226 [00:49<07:31,  2.29s/it]\u001b[A\n",
      " 13%|██████████▊                                                                      | 30/226 [00:50<06:17,  1.93s/it]\u001b[A\n",
      " 14%|███████████                                                                      | 31/226 [00:51<05:31,  1.70s/it]\u001b[A\n",
      " 14%|███████████▍                                                                     | 32/226 [00:52<04:51,  1.50s/it]\u001b[A\n",
      " 15%|███████████▊                                                                     | 33/226 [00:54<05:06,  1.59s/it]\u001b[A\n",
      " 15%|████████████▏                                                                    | 34/226 [00:56<05:26,  1.70s/it]\u001b[A\n",
      " 15%|████████████▌                                                                    | 35/226 [00:57<04:55,  1.55s/it]\u001b[A\n",
      " 16%|████████████▉                                                                    | 36/226 [00:59<05:20,  1.69s/it]\u001b[A\n",
      " 16%|█████████████▎                                                                   | 37/226 [01:01<05:12,  1.66s/it]\u001b[A\n",
      " 17%|█████████████▌                                                                   | 38/226 [01:02<04:53,  1.56s/it]\u001b[A\n",
      " 17%|█████████████▉                                                                   | 39/226 [01:03<03:45,  1.21s/it]\u001b[A\n",
      " 18%|██████████████▎                                                                  | 40/226 [01:04<03:54,  1.26s/it]\u001b[A\n",
      " 18%|██████████████▋                                                                  | 41/226 [01:05<04:03,  1.31s/it]\u001b[A\n",
      " 19%|███████████████                                                                  | 42/226 [01:07<04:01,  1.31s/it]\u001b[A\n",
      " 19%|███████████████▍                                                                 | 43/226 [01:09<04:28,  1.47s/it]\u001b[A\n",
      " 19%|███████████████▊                                                                 | 44/226 [01:09<03:24,  1.13s/it]\u001b[A\n",
      " 20%|████████████████▏                                                                | 45/226 [01:09<02:56,  1.02it/s]\u001b[A\n",
      " 20%|████████████████▍                                                                | 46/226 [01:11<03:22,  1.13s/it]\u001b[A\n",
      " 21%|████████████████▊                                                                | 47/226 [01:13<03:57,  1.33s/it]\u001b[A\n",
      " 21%|█████████████████▏                                                               | 48/226 [01:13<02:58,  1.00s/it]\u001b[A\n",
      " 22%|█████████████████▌                                                               | 49/226 [01:14<02:52,  1.03it/s]\u001b[A\n",
      " 22%|█████████████████▉                                                               | 50/226 [01:15<03:11,  1.09s/it]\u001b[A\n",
      " 23%|██████████████████▎                                                              | 51/226 [01:17<03:23,  1.16s/it]\u001b[A\n",
      " 23%|██████████████████▋                                                              | 52/226 [01:17<03:00,  1.04s/it]\u001b[A\n",
      " 23%|██████████████████▉                                                              | 53/226 [01:18<02:23,  1.21it/s]\u001b[A\n",
      " 24%|███████████████████▎                                                             | 54/226 [01:20<03:30,  1.22s/it]\u001b[A\n",
      " 24%|███████████████████▋                                                             | 55/226 [01:21<03:39,  1.28s/it]\u001b[A\n",
      " 25%|████████████████████                                                             | 56/226 [01:23<03:47,  1.34s/it]\u001b[A\n",
      " 25%|████████████████████▍                                                            | 57/226 [01:23<03:15,  1.16s/it]\u001b[A\n",
      " 26%|████████████████████▊                                                            | 58/226 [01:24<02:44,  1.02it/s]\u001b[A\n",
      " 26%|█████████████████████▏                                                           | 59/226 [01:25<02:52,  1.03s/it]\u001b[A\n",
      " 27%|█████████████████████▌                                                           | 60/226 [01:26<02:55,  1.06s/it]\u001b[A\n",
      " 27%|█████████████████████▊                                                           | 61/226 [01:28<03:11,  1.16s/it]\u001b[A\n",
      " 27%|██████████████████████▏                                                          | 62/226 [01:29<03:30,  1.28s/it]\u001b[A\n",
      " 28%|██████████████████████▌                                                          | 63/226 [01:32<04:39,  1.72s/it]\u001b[A\n",
      " 28%|██████████████████████▉                                                          | 64/226 [01:35<05:35,  2.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▎                                                         | 65/226 [01:36<05:06,  1.91s/it]\u001b[A\n",
      " 29%|███████████████████████▋                                                         | 66/226 [01:38<05:02,  1.89s/it]\u001b[A\n",
      " 30%|████████████████████████                                                         | 67/226 [01:40<05:10,  1.96s/it]\u001b[A\n",
      " 30%|████████████████████████▎                                                        | 68/226 [01:41<03:50,  1.46s/it]\u001b[A\n",
      " 31%|████████████████████████▋                                                        | 69/226 [01:42<03:54,  1.49s/it]\u001b[A\n",
      " 31%|█████████████████████████                                                        | 70/226 [01:43<03:25,  1.31s/it]\u001b[A\n",
      " 31%|█████████████████████████▍                                                       | 71/226 [01:44<02:46,  1.07s/it]\u001b[A\n",
      " 32%|█████████████████████████▊                                                       | 72/226 [01:45<02:46,  1.08s/it]\u001b[A\n",
      " 32%|██████████████████████████▏                                                      | 73/226 [01:46<02:57,  1.16s/it]\u001b[A\n",
      " 33%|██████████████████████████▌                                                      | 74/226 [01:50<05:23,  2.13s/it]\u001b[A\n",
      " 33%|██████████████████████████▉                                                      | 75/226 [01:51<03:58,  1.58s/it]\u001b[A\n",
      " 34%|███████████████████████████▏                                                     | 76/226 [01:51<03:10,  1.27s/it]\u001b[A\n",
      " 34%|███████████████████████████▌                                                     | 77/226 [01:54<04:05,  1.65s/it]\u001b[A\n",
      " 35%|███████████████████████████▉                                                     | 78/226 [01:56<04:05,  1.66s/it]\u001b[A\n",
      " 35%|████████████████████████████▎                                                    | 79/226 [01:57<04:02,  1.65s/it]\u001b[A\n",
      " 35%|████████████████████████████▋                                                    | 80/226 [01:59<03:54,  1.61s/it]\u001b[A\n",
      " 36%|█████████████████████████████                                                    | 81/226 [01:59<02:56,  1.22s/it]\u001b[A\n",
      " 36%|█████████████████████████████▍                                                   | 82/226 [02:01<03:26,  1.43s/it]\u001b[A\n",
      " 37%|█████████████████████████████▋                                                   | 83/226 [02:01<02:40,  1.12s/it]\u001b[A\n",
      " 37%|██████████████████████████████                                                   | 84/226 [02:02<02:08,  1.10it/s]\u001b[A\n",
      " 38%|██████████████████████████████▍                                                  | 85/226 [02:03<02:28,  1.05s/it]\u001b[A\n",
      " 38%|██████████████████████████████▊                                                  | 86/226 [02:04<02:17,  1.02it/s]\u001b[A\n",
      " 38%|███████████████████████████████▏                                                 | 87/226 [02:05<02:24,  1.04s/it]\u001b[A\n",
      " 39%|███████████████████████████████▌                                                 | 88/226 [02:08<03:51,  1.68s/it]\u001b[A\n",
      " 39%|███████████████████████████████▉                                                 | 89/226 [02:10<03:45,  1.64s/it]\u001b[A\n",
      " 40%|████████████████████████████████▎                                                | 90/226 [02:10<02:52,  1.27s/it]\u001b[A\n",
      " 40%|████████████████████████████████▌                                                | 91/226 [02:11<02:17,  1.02s/it]\u001b[A\n",
      " 41%|████████████████████████████████▉                                                | 92/226 [02:12<02:33,  1.14s/it]\u001b[A\n",
      " 41%|█████████████████████████████████▎                                               | 93/226 [02:13<02:32,  1.15s/it]\u001b[A\n",
      " 42%|█████████████████████████████████▋                                               | 94/226 [02:14<01:58,  1.11it/s]\u001b[A\n",
      " 42%|██████████████████████████████████                                               | 95/226 [02:15<01:59,  1.10it/s]\u001b[A\n",
      " 42%|██████████████████████████████████▍                                              | 96/226 [02:16<02:36,  1.21s/it]\u001b[A\n",
      " 43%|██████████████████████████████████▊                                              | 97/226 [02:19<03:12,  1.49s/it]\u001b[A\n",
      " 43%|███████████████████████████████████                                              | 98/226 [02:20<03:09,  1.48s/it]\u001b[A\n",
      " 44%|███████████████████████████████████▍                                             | 99/226 [02:22<03:09,  1.49s/it]\u001b[A\n",
      " 44%|███████████████████████████████████▍                                            | 100/226 [02:22<02:44,  1.31s/it]\u001b[A\n",
      " 45%|███████████████████████████████████▊                                            | 101/226 [02:24<02:40,  1.28s/it]\u001b[A\n",
      " 45%|████████████████████████████████████                                            | 102/226 [02:24<02:10,  1.05s/it]\u001b[A\n",
      " 46%|████████████████████████████████████▍                                           | 103/226 [02:26<02:33,  1.25s/it]\u001b[A\n",
      " 46%|████████████████████████████████████▊                                           | 104/226 [02:28<03:08,  1.55s/it]\u001b[A\n",
      " 46%|█████████████████████████████████████▏                                          | 105/226 [02:30<03:23,  1.69s/it]\u001b[A\n",
      " 47%|█████████████████████████████████████▌                                          | 106/226 [02:32<03:42,  1.86s/it]\u001b[A\n",
      " 47%|█████████████████████████████████████▉                                          | 107/226 [02:35<04:10,  2.11s/it]\u001b[A\n",
      " 48%|██████████████████████████████████████▏                                         | 108/226 [02:37<03:51,  1.96s/it]\u001b[A\n",
      " 48%|██████████████████████████████████████▌                                         | 109/226 [02:37<02:52,  1.47s/it]\u001b[A\n",
      " 49%|██████████████████████████████████████▉                                         | 110/226 [02:38<02:51,  1.47s/it]\u001b[A\n",
      " 49%|███████████████████████████████████████▎                                        | 111/226 [02:40<02:52,  1.50s/it]\u001b[A\n",
      " 50%|███████████████████████████████████████▋                                        | 112/226 [02:41<02:45,  1.45s/it]\u001b[A\n",
      " 50%|████████████████████████████████████████                                        | 113/226 [02:42<02:07,  1.13s/it]\u001b[A\n",
      " 50%|████████████████████████████████████████▎                                       | 114/226 [02:43<02:15,  1.21s/it]\u001b[A\n",
      " 51%|████████████████████████████████████████▋                                       | 115/226 [02:44<01:49,  1.02it/s]\u001b[A\n",
      " 51%|█████████████████████████████████████████                                       | 116/226 [02:44<01:44,  1.05it/s]\u001b[A\n",
      " 52%|█████████████████████████████████████████▍                                      | 117/226 [02:46<02:00,  1.10s/it]\u001b[A\n",
      " 52%|█████████████████████████████████████████▊                                      | 118/226 [02:46<01:39,  1.08it/s]\u001b[A\n",
      " 53%|██████████████████████████████████████████                                      | 119/226 [02:48<01:44,  1.02it/s]\u001b[A\n",
      " 53%|██████████████████████████████████████████▍                                     | 120/226 [02:48<01:23,  1.28it/s]\u001b[A\n",
      " 54%|██████████████████████████████████████████▊                                     | 121/226 [02:50<01:49,  1.05s/it]\u001b[A\n",
      " 54%|███████████████████████████████████████████▏                                    | 122/226 [02:51<02:00,  1.16s/it]\u001b[A\n",
      " 54%|███████████████████████████████████████████▌                                    | 123/226 [02:53<02:18,  1.34s/it]\u001b[A\n",
      " 55%|███████████████████████████████████████████▉                                    | 124/226 [02:53<01:51,  1.09s/it]\u001b[A\n",
      " 55%|████████████████████████████████████████████▏                                   | 125/226 [02:55<02:14,  1.34s/it]\u001b[A\n",
      " 56%|████████████████████████████████████████████▌                                   | 126/226 [02:58<02:49,  1.70s/it]\u001b[A\n",
      " 56%|████████████████████████████████████████████▉                                   | 127/226 [02:59<02:35,  1.57s/it]\u001b[A\n",
      " 57%|█████████████████████████████████████████████▎                                  | 128/226 [03:01<02:34,  1.57s/it]\u001b[A\n",
      " 57%|█████████████████████████████████████████████▋                                  | 129/226 [03:02<02:19,  1.44s/it]\u001b[A\n",
      " 58%|██████████████████████████████████████████████                                  | 130/226 [03:02<01:49,  1.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████▎                                 | 131/226 [03:02<01:24,  1.12it/s]\u001b[A\n",
      " 58%|██████████████████████████████████████████████▋                                 | 132/226 [03:03<01:11,  1.31it/s]\u001b[A\n",
      " 59%|███████████████████████████████████████████████                                 | 133/226 [03:05<01:44,  1.13s/it]\u001b[A\n",
      " 59%|███████████████████████████████████████████████▍                                | 134/226 [03:05<01:24,  1.09it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████████▊                                | 135/226 [03:06<01:14,  1.23it/s]\u001b[A\n",
      " 60%|████████████████████████████████████████████████▏                               | 136/226 [03:08<01:38,  1.09s/it]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▍                               | 137/226 [03:08<01:27,  1.02it/s]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▊                               | 138/226 [03:09<01:07,  1.31it/s]\u001b[A\n",
      " 62%|█████████████████████████████████████████████████▏                              | 139/226 [03:09<00:55,  1.58it/s]\u001b[A\n",
      " 62%|█████████████████████████████████████████████████▌                              | 140/226 [03:12<01:50,  1.29s/it]\u001b[A\n",
      " 62%|█████████████████████████████████████████████████▉                              | 141/226 [03:14<02:22,  1.67s/it]\u001b[A\n",
      " 63%|██████████████████████████████████████████████████▎                             | 142/226 [03:15<01:49,  1.30s/it]\u001b[A\n",
      " 63%|██████████████████████████████████████████████████▌                             | 143/226 [03:17<02:20,  1.69s/it]\u001b[A\n",
      " 64%|██████████████████████████████████████████████████▉                             | 144/226 [03:18<01:48,  1.33s/it]\u001b[A\n",
      " 64%|███████████████████████████████████████████████████▎                            | 145/226 [03:20<01:56,  1.44s/it]\u001b[A\n",
      " 65%|███████████████████████████████████████████████████▋                            | 146/226 [03:21<02:01,  1.52s/it]\u001b[A\n",
      " 65%|████████████████████████████████████████████████████                            | 147/226 [03:23<01:57,  1.49s/it]\u001b[A\n",
      " 65%|████████████████████████████████████████████████████▍                           | 148/226 [03:24<01:59,  1.53s/it]\u001b[A\n",
      " 66%|████████████████████████████████████████████████████▋                           | 149/226 [03:26<01:55,  1.50s/it]\u001b[A\n",
      " 66%|█████████████████████████████████████████████████████                           | 150/226 [03:27<01:41,  1.33s/it]\u001b[A\n",
      " 67%|█████████████████████████████████████████████████████▍                          | 151/226 [03:28<01:43,  1.37s/it]\u001b[A\n",
      " 67%|█████████████████████████████████████████████████████▊                          | 152/226 [03:29<01:35,  1.29s/it]\u001b[A\n",
      " 68%|██████████████████████████████████████████████████████▏                         | 153/226 [03:31<01:46,  1.47s/it]\u001b[A\n",
      " 68%|██████████████████████████████████████████████████████▌                         | 154/226 [03:33<01:54,  1.59s/it]\u001b[A\n",
      " 69%|██████████████████████████████████████████████████████▊                         | 155/226 [03:33<01:26,  1.21s/it]\u001b[A\n",
      " 69%|███████████████████████████████████████████████████████▏                        | 156/226 [03:35<01:29,  1.28s/it]\u001b[A\n",
      " 69%|███████████████████████████████████████████████████████▌                        | 157/226 [03:37<01:54,  1.66s/it]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████████▉                        | 158/226 [03:38<01:42,  1.51s/it]\u001b[A\n",
      " 70%|████████████████████████████████████████████████████████▎                       | 159/226 [03:40<01:44,  1.56s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# 키워드 추출\n",
    "kwd_list = makeKeywordLsit(data, nn_jj = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d665577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 추가\n",
    "data['키워드'] = kwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eaedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slop(df2,from_date,to_date):\n",
    "    datelist = pd.date_range(start=from_date, end=to_date, freq = 'M').tolist()\n",
    "    dtlst = []\n",
    "    df = df2.copy()\n",
    "    for d_t in datelist:\n",
    "        d_t = str(d_t)[0:-9]\n",
    "        d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "        d = d.strftime('%Y%m')\n",
    "        dtlst.append(d)\n",
    "    for dt in dtlst:\n",
    "        df[dt] = df[dt][:-1]*10000 / df[dt][-1]\n",
    "    from scipy import stats\n",
    "\n",
    "    x = [x + 1for x in range(0,len(dtlst))]\n",
    "\n",
    "    slopes = []\n",
    "    for i in list(df.values):\n",
    "        y = i[:-2]\n",
    "        slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
    "        slopes.append(slope)\n",
    "    df['slope'] = slopes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pro(data,from_date,to_date):\n",
    "\n",
    "    df = data[['일자','키워드']]\n",
    "    df['일자'] = [str(x)[0:6] for x in df['일자']] \n",
    "    # Input string\n",
    "    date_str = to_date\n",
    "\n",
    "    # Convert the string to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "    # Add one month to the datetime object\n",
    "    new_date_obj = date_obj + relativedelta(months=1)\n",
    "\n",
    "    # Convert the result back to a string\n",
    "    new_date_str = new_date_obj.strftime('%Y%m%d')\n",
    "    to_date = new_date_str\n",
    "    datelist = pd.date_range(start=from_date, end=to_date, freq = 'M').tolist()\n",
    "    dtlst = []\n",
    "\n",
    "    for d_t in datelist:\n",
    "        d_t = str(d_t)[0:-9]\n",
    "        d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "        d = d.strftime('%Y%m')\n",
    "        dtlst.append(d)\n",
    "\n",
    "    # 키워드 사전 \n",
    "    keyword_vocab = []\n",
    "    for i in df['키워드']:\n",
    "        keyword_vocab.extend(i)\n",
    "    keyword_vocab = list(set(keyword_vocab))\n",
    "\n",
    "    # 월별 카운팅 후 병합 # 병합하는 방식 수정해야함. 한달만 카운트는 현재 불가.\n",
    "\n",
    "    tmp_df_list = []\n",
    "    for dt in dtlst:\n",
    "        tmp = []\n",
    "        for i in df[df['일자']== dt]['키워드']:\n",
    "            tmp.extend(i)\n",
    "        tmp_df = pd.DataFrame(pd.Series(Counter(tmp)))\n",
    "        tmp_df = tmp_df.reset_index()\n",
    "        tmp_df = tmp_df.rename(columns = {0:'cnt','index':'keyword'})\n",
    "        tmp_df_list.append(tmp_df)\n",
    "\n",
    "    if len(tmp_df_list) > 1:\n",
    "        df = tmp_df_list[0].merge(tmp_df_list[1], on = 'keyword', how = 'outer')\n",
    "        for i in range(len(tmp_df_list)):\n",
    "            if i > 1:\n",
    "                df = df.merge(tmp_df_list[i], on = 'keyword', how = 'outer')\n",
    "        col = ['keyword'] + dtlst\n",
    "        df.columns = col\n",
    "    elif len(tmp_df_list) == 1:\n",
    "        df = tmp_df_list[0]\n",
    "        col = ['keyword'] + dtlst\n",
    "        df.columns = col\n",
    "\n",
    "    # 합산 값 만들기\n",
    "    df.index = df['keyword']\n",
    "    df = df.drop('keyword', axis = 1)\n",
    "    df['tot_cnt'] = df.sum(axis = 1)\n",
    "    df = df.T\n",
    "    df['year_cnt'] = df.sum(axis = 1)\n",
    "    df = df.T\n",
    "    df\n",
    "\n",
    "    # 결측치 처리\n",
    "    df =df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139aa3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataEn(topic,from_date,to_date,slope = True):\n",
    "    cwd = os.getcwd()\n",
    "    main_path = cwd + '/' + topic\n",
    "    name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "    name = name.replace('/','.')\n",
    "    with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "        data_e = pickle.load(f)\n",
    "      \n",
    "    result = data_e[['일자','키워드']].explode('키워드').reset_index(drop=True)\n",
    "    data_e = result\n",
    "    print(\"총 기사 건 수 : \" + str(len(data_e['키워드'])))\n",
    "    df_e = data_pro(data_e,from_date,to_date)\n",
    "    df_e.to_csv(cwd +'/' + topic + '/'+ 'Monthly_Count_Keyword {}-{}.csv'.format(from_date,to_date),encoding=\"utf-8-sig\")\n",
    "    \n",
    "    if slope == True:\n",
    "        df_es = get_slop(df_e,from_date,to_date)\n",
    "        df_es.to_csv(cwd +'/' + topic + '/'+ 'Efficient_OLS_Keyword {}-{}.csv'.format(from_date,to_date),encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataEn(searching_word,from_date,to_date,slope = False) # 한달이내는 기울기를 만들 수 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# 워드 클라우드\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from wordcloud import (WordCloud, get_single_color_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawPointColoredWC(tags,title,cwd,topic, drop_list=[], color = '#00ff00', pointed_list=[]):\n",
    "    color_to_words = {\n",
    "        color : pointed_list\n",
    "    }\n",
    "\n",
    "    use_tags = tags.drop(drop_list, errors = 'ignore')\n",
    "    wc = WordCloud(font_path='C:/Windows/Fonts/malgun',background_color=\"white\",width=1600, height=800,random_state = 1)\n",
    "\n",
    "    default_color = 'black'\n",
    "    grouped_color_func = GroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "    cloud = wc.generate_from_frequencies(dict(use_tags))\n",
    "    wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "    figure(dpi=1200)\n",
    "    figure(figsize=[12,8])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.savefig(cwd + '/' + topic + '/'+ title + ' 강조'+'.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d746c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n",
    "\n",
    "\n",
    "class GroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n",
    "       specified colors to certain words based on the color to words mapping.\n",
    "\n",
    "       Uses wordcloud.get_single_color_func\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94de907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "cwd = os.getcwd()\n",
    "path = cwd +'/' + topic + '/raw'\n",
    "\n",
    "df_cnt = pd.read_csv(cwd +'/' + topic + '/'+ 'Monthly_Count_Keyword {}-{}.csv'.format(from_date,to_date),index_col = 'keyword')\n",
    "\n",
    "main_path = cwd + '/' + topic\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "    data_e = pickle.load(f)\n",
    "    \n",
    "result = data_e[['일자','url']].explode('url').reset_index(drop=True)\n",
    "url_long_data = result.explode('url').reset_index(drop=True)\n",
    "\n",
    "keyword_long_data = data_e[['일자','키워드']].explode('키워드').reset_index(drop=True)\n",
    "\n",
    "print('{}로 검색하여 {}부터 {}까지 {} 건의 기사를 인용했습니다.'.format(topic,from_date,to_date,len(url_long_data )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 키워드 수, 지울 단어, 강조할 단어 지정\n",
    "num_exp = 50\n",
    "drop_list = [np.nan,'thomson','review','service','supports','review terms','sure','information','reuters','policy','terms','theres','happen','cookies','content','javascript','average','sector','sectors','standards','trust','browser','supports javascript','did happen','loading information']\n",
    "num = num_exp + len(drop_list)\n",
    "color_to_words = {\n",
    "    '#00ff00': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51239ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그림 그리기 및 저장\n",
    "print('{}에서 {}까지 키워드 출현 빈도 순 상위 {}개 입니다'.format(from_date,to_date,num_exp))\n",
    "tags = df_cnt.sort_values(by = 'tot_cnt', ascending = False)[1:num+1]['tot_cnt']\n",
    "tags = tags.drop(drop_list, errors = 'ignore')\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun',background_color=\"white\",width=1600, height=800)\n",
    "# default_color = 'black'\n",
    "# grouped_color_func = GroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "cloud = wc.generate_from_frequencies(dict(tags))\n",
    "# wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "figure(dpi=1200)\n",
    "title = '대상기간_최대출현_키워드_{}개'.format(str(num_exp))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.savefig(cwd + '/' + topic + '/'+ title + from_date + '.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31858518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in keyword_long_data['키워드']:\n",
    "    if keyword in i:\n",
    "        tmp.append(True)\n",
    "    else:\n",
    "        tmp.append(False)\n",
    "\n",
    "for url in url_long_data[tmp]['url']:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6495b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 하루씩 개별로 수집하고 이를 일주일단위로 묶어서 df를 만드는 db구조를 만들어야할듯? 우선은 주먹구구식으로 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a80be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta as td\n",
    "from datetime import date as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "today = dt.today()\n",
    "to_date = today - td(days = 2)\n",
    "from_date  =  today - td(days = 8)\n",
    "\n",
    "# dd/mm/YY\n",
    "to_date = to_date.strftime(\"%Y%m%d\")\n",
    "from_date = from_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path = cwd +'/' + topic + '/raw'\n",
    "main_path = cwd + '/' + topic\n",
    "\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "    data_e_past = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ee96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e_past = pd.concat([data_e_past,data_e]).reset_index(drop = True)[1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ad9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.today()\n",
    "to_date = today - td(days = 1)\n",
    "from_date  =  today - td(days = 7)\n",
    "\n",
    "# dd/mm/YY\n",
    "to_date = to_date.strftime(\"%Y%m%d\")\n",
    "from_date = from_date.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name,'wb') as f:\n",
    "    pickle.dump(data_e_past,f)\n",
    "    \n",
    "with open(main_path + '/' + 'keyword/' + name, 'rb') as f:\n",
    "    data_e_past = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649daf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DataEn(searching_word,from_date ,to_date ,slope = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0054a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_past = pd.read_csv(cwd +'/' + topic + '/'+ 'Monthly_Count_Keyword {}-{}.csv'.format(from_date,to_date),index_col = 'keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55644a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_e_past[['일자','url']].explode('url').reset_index(drop=True)\n",
    "url_long_data_past = result.explode('url').reset_index(drop=True)\n",
    "\n",
    "keyword_long_data_past = data_e[['일자','키워드']].explode('키워드').reset_index(drop=True)\n",
    "\n",
    "print('{}로 검색하여 {}부터 {}까지 {} 건의 기사를 인용했습니다.'.format(topic,from_date,to_date,len(url_long_data_past )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717747ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_past_mean = df_cnt_past / 7 \n",
    "past_count = df_cnt_past_mean .sort_values(by = 'tot_cnt', ascending = False)['tot_cnt']\n",
    "past_count.name = 'past_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37724298",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_count =  df_cnt.sort_values(by = 'tot_cnt', ascending = False)[0:51]['tot_cnt']\n",
    "today_count.name = 'today_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare\n",
    "result = pd.concat([today_count,past_count], axis=1, join='outer', ignore_index=False).fillna(0)\n",
    "# result['mean'] = result.mean(axis = 1)\n",
    "\n",
    "# divisor = 'mean'\n",
    "\n",
    "# new_df = result.div(result[divisor], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b673540",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['today_index'] = (result['today_count'] - result['past_count'])*100 / result['past_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579331c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = '오늘 키워드 강조'\n",
    "pointedTag_cost = result[result['today_index'] > 10].sort_values(by = 'today_index',ascending = False)[:25]\n",
    "DrawPointColoredWC(tags = tags, title =title,cwd = cwd,topic = 'economy',drop_list = drop_list,pointed_list=pointedTag_cost.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7bef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pointedTag_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in keyword_long_data['키워드']:\n",
    "    if keyword in i:\n",
    "        tmp.append(True)\n",
    "    else:\n",
    "        tmp.append(False)\n",
    "\n",
    "for url in url_long_data[tmp]['url']:\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
